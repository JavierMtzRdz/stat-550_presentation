@misc{8Sex:2024,
  title = {882: {{Significant}} - Explain Xkcd},
  urldate = {2024-03-19},
  howpublished = {https://www.explainxkcd.com/wiki/index.php/882:\_Significant},
  file = {/Users/javiermtz/Zotero/storage/TGH8YZJT/882_Significant.html}
}

@article{AltmanBland:1995,
  title = {Statistics Notes: {{Absence}} of Evidence Is Not Evidence of Absence},
  shorttitle = {Statistics Notes},
  author = {Altman, Douglas G. and Bland, J. Martin},
  year = {1995},
  month = aug,
  journal = {BMJ},
  volume = {311},
  number = {7003},
  pages = {485},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.311.7003.485},
  urldate = {2024-03-19},
  abstract = {The non-equivalence of statistical significance and clinical importance has long been recognised, but this error of interpretation remains common. Although a significant result in a large study may sometimes not be clinically important, a far greater problem arises from misinterpretation of non-significant findings. By convention a P value greater than 5\% (P{$>$}0.05) is called ``not significant.'' Randomised controlled clinical trials that do not show a significant difference between the treatments being compared are often called ``negative.'' This term wrongly implies that the study has shown that there is no difference, whereas usually all that has been shown is an absence of evidence of a difference. These are quite different statements. The sample size of controlled trials is generally inadequate, with a consequent lack of power to detect real, and clinically worthwhile, differences in treatment. Freiman et al1 found that only {\dots}},
  chapter = {Paper},
  copyright = {{\copyright} 1995 BMJ Publishing Group Ltd.},
  langid = {english},
  pmid = {7647644}
}

@article{AmrheinGreenlandMcShane:2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  year = {2019},
  month = mar,
  journal = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-019-00857-9},
  urldate = {2024-03-19},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Research data,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Research data, Research management},
  file = {/Users/javiermtz/Zotero/storage/R4JPLI66/d41586-019-00857-9.html}
}

@misc{Aschwanden:2015,
  title = {Science {{Isn}}'t {{Broken}}},
  author = {Aschwanden, Christie},
  year = {2015},
  month = aug,
  journal = {FiveThirtyEight},
  urldate = {2024-03-20},
  abstract = {If you follow the headlines, your confidence in science may have taken a hit lately. Peer review? More like self-review. An investigation in November uncovered a scam in which researchers were rubber-stamping their own work, circumventing peer review at five high-profile publishers.},
  langid = {american},
  file = {/Users/javiermtz/Zotero/storage/CUN4G98H/science-isnt-broken.html}
}

@misc{Aschwanden:2016,
  title = {You {{Can}}'t {{Trust What You Read About Nutrition}}},
  author = {Aschwanden, Christie},
  year = {2016},
  month = jan,
  journal = {FiveThirtyEight},
  urldate = {2024-03-19},
  abstract = {Photographs by Anna Maria Barry-Jester As the new year begins, millions of people are vowing to shape up their eating habits. This usually involves dividing foo{\dots}},
  langid = {american},
  file = {/Users/javiermtz/Zotero/storage/WKXY3Y92/you-cant-trust-what-you-read-about-nutrition.html}
}

@article{Baker:2016,
  title = {Statisticians Issue Warning over Misuse of {{P}} Values},
  author = {Baker, Monya},
  year = {2016},
  month = mar,
  journal = {Nature},
  volume = {531},
  number = {7593},
  pages = {151--151},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature.2016.19503},
  urldate = {2024-03-19},
  abstract = {Policy statement aims to halt missteps in the quest for certainty.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Peer review,Policy,Publishing,Research data}
}

@misc{Barnett:2022,
  title = {Bad Statistics in Medical Research},
  author = {Barnett, Adrian},
  year = {2022},
  month = sep,
  urldate = {2024-03-18},
  abstract = {If you use p=0.05 to suggest that you have made a discovery, you will be wrong at least 30\% of the time. If, as is often the case, experiments are underpowered, you will be wrong most of the time. This conclusion is demonstrated from several points of view. First, tree diagrams which show the close analogy with the screening test problem. Similar conclusions are drawn by repeated simulations of t-tests. These mimic what is done in real life, which makes the results more persuasive. The simulation method is used also to evaluate the extent to which effect sizes are over-estimated, especially in underpowered experiments. A script is supplied to allow the reader to do simulations themselves, with numbers appropriate for their own work. It is concluded that if you wish to keep your false discovery rate below 5\%, you need to use a three-sigma rule, or to insist on p{$\leq$}0.001. And never use the word `significant'.},
  keywords = {false discovery rate,reproducibility,significance tests,statistics}
}

@article{BenjaminBerger:2019,
  title = {Three {{Recommendations}} for {{Improving}} the {{Use}} of P-{{Values}}},
  author = {Benjamin, Daniel J. and Berger, James O.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {186--191},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1543135},
  urldate = {2024-03-20},
  abstract = {Researchers commonly use p-values to answer the question: How strongly does the evidence favor the alternative hypothesis relative to the null hypothesis? p-Values themselves do not directly answer this question and are often misinterpreted in ways that lead to overstating the evidence against the null hypothesis. Even in the ``post p {$<$} 0.05 era,'' however, it is quite possible that p-values will continue to be widely reported and used to assess the strength of evidence (if for no other reason than the widespread availability and use of statistical software that routinely produces p-values and thereby implicitly advocates for their use). If so, the potential for misinterpretation will persist. In this article, we recommend three practices that would help researchers more accurately interpret p-values. Each of the three recommended practices involves interpreting p-values in light of their corresponding ``Bayes factor bound,'' which is the largest odds in favor of the alternative hypothesis relative to the null hypothesis that is consistent with the observed data. The Bayes factor bound generally indicates that a given p-value provides weaker evidence against the null hypothesis than typically assumed. We therefore believe that our recommendations can guard against some of the most harmful p-value misinterpretations. In research communities that are deeply attached to reliance on ``p {$<$} 0.05,'' our recommendations will serve as initial steps away from this attachment. We emphasize that our recommendations are intended merely as initial, temporary steps and that many further steps will need to be taken to reach the ultimate destination: a holistic interpretation of statistical evidence that fully conforms to the principles laid out in the ASA statement on statistical significance and p-values.},
  keywords = {Bayes factor,p-Value,Post-experimental odds}
}

@article{BernerAmrhein:2022,
  title = {Why and How We Should Join the Shift from Significance Testing to Estimation},
  author = {Berner, Daniel and Amrhein, Valentin},
  year = {2022},
  month = jun,
  journal = {Journal of Evolutionary Biology},
  volume = {35},
  number = {6},
  pages = {777},
  publisher = {Wiley-Blackwell},
  doi = {10.1111/jeb.14009},
  urldate = {2024-03-20},
  abstract = {A paradigm shift away from null hypothesis significance testing seems in progress. Based on simulations, we illustrate some of the underlying motivations. First, p-values vary strongly from study to study, hence dichotomous inference using significance ...},
  langid = {english},
  pmid = {35582935},
  file = {/Users/javiermtz/Zotero/storage/CUM3ICJ6/PMC9322409.html}
}

@article{BlumeGreevyWelty:2019,
  title = {An {{Introduction}} to {{Second-Generation}} p-{{Values}}},
  author = {Blume, Jeffrey D. and Greevy, Robert A. and Welty, Valerie F. and Smith, Jeffrey R. and Dupont, William D.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {157--167},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537893},
  urldate = {2024-03-20},
  abstract = {Second generation p-values preserve the simplicity that has made p-values popular while resolving critical flaws that promote misinterpretation of data, distraction by trivial effects, and unreproducible assessments of data. The second-generation p-value (SGPV) is an extension that formally accounts for scientific relevance by using a composite null hypothesis that captures null and scientifically trivial effects. Because the majority of spurious findings are small effects that are technically nonnull but practically indistinguishable from the null, the second-generation approach greatly reduces the likelihood of a false discovery. SGPVs promote transparency, rigor and reproducibility of scientific results by a priori identifying which candidate hypotheses are practically meaningful and by providing a more reliable statistical summary of when the data are compatible with the candidate hypotheses or null hypotheses, or when the data are inconclusive. We illustrate the importance of these advances using a dataset of 247,000 single-nucleotide polymorphisms, i.e., genetic markers that are potentially associated with prostate cancer.},
  keywords = {Likelihood ratios,Null hypothesis,p-Value,Statistical evidence}
}

@misc{Bohannon:2015,
  title = {I {{Fooled Millions Into Thinking Chocolate Helps Weight Loss}}. {{Here}}'s {{How}}.},
  author = {Bohannon, John},
  year = {2015},
  month = may,
  journal = {Gizmodo},
  urldate = {2024-03-20},
  abstract = {``Slim by Chocolate!'' the headlines blared. A team of German researchers had found that people on a low-carb diet lost weight 10 percent faster if they ate a chocolate bar every day. It made the front page of Bild, Europe's largest daily newspaper, just beneath their update about the Germanwings crash. From there, it{\dots}},
  howpublished = {https://gizmodo.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/BBZLJ9Q3/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800.html}
}

@article{BrodeurCookHeyes:2020,
  title = {Methods {{Matter}}: P-{{Hacking}} and {{Publication Bias}} in {{Causal Analysis}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2020},
  month = nov,
  journal = {American Economic Review},
  volume = {110},
  number = {11},
  pages = {3634--3660},
  issn = {0002-8282},
  doi = {10.1257/aer.20190687},
  urldate = {2024-03-19},
  abstract = {The credibility revolution in economics has promoted causal identification using randomized control trials (RCT), difference-in-differences (DID), instrumental variables (IV) and regression discontinuity design (RDD). Applying multiple approaches to over 21,000 hypothesis tests published in 25 leading economics journals, we find that the extent of p-hacking and publication bias varies greatly by method. IV (and to a lesser extent DID) are particularly problematic. We find no evidence that (i) papers published in the Top 5 journals are different to others; (ii) the journal "revise and resubmit" process mitigates the problem; (iii) things are improving through time.},
  langid = {english},
  keywords = {and Selection,Hypothesis Testing: General,Model Evaluation,Sociology of Economics,Validation}
}

@article{CacioppoCacioppoGonzaga:2013,
  title = {Marital Satisfaction and Break-Ups Differ across on-Line and off-Line Meeting Venues},
  author = {Cacioppo, John T. and Cacioppo, Stephanie and Gonzaga, Gian C. and Ogburn, Elizabeth L. and VanderWeele, Tyler J.},
  year = {2013},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {25},
  pages = {10135--10140},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1222447110},
  urldate = {2024-03-20},
  abstract = {Marital discord is costly to children, families, and communities. The advent of the Internet, social networking, and on-line dating has affected how people meet future spouses, but little is known about the prevalence or outcomes of these marriages or the demographics of those involved. We addressed these questions in a nationally representative sample of 19,131 respondents who married between 2005 and 2012. Results indicate that more than one-third of marriages in America now begin on-line. In addition, marriages that began on-line, when compared with those that began through traditional off-line venues, were slightly less likely to result in a marital break-up (separation or divorce) and were associated with slightly higher marital satisfaction among those respondents who remained married. Demographic differences were identified between respondents who met their spouse through on-line vs. traditional off-line venues, but the findings for marital break-up and marital satisfaction remained significant after statistically controlling for these differences. These data suggest that the Internet may be altering the dynamics and outcomes of marriage itself.}
}

@article{Colquhoun:2014,
  title = {An Investigation of the False Discovery Rate and the Misinterpretation of P-Values},
  author = {Colquhoun, David},
  year = {2014},
  month = nov,
  journal = {Royal Society Open Science},
  volume = {1},
  number = {3},
  pages = {140216},
  publisher = {Royal Society},
  doi = {10.1098/rsos.140216},
  urldate = {2024-03-19},
  abstract = {If you use p=0.05 to suggest that you have made a discovery, you will be wrong at least 30\% of the time. If, as is often the case, experiments are underpowered, you will be wrong most of the time. This conclusion is demonstrated from several points of view. First, tree diagrams which show the close analogy with the screening test problem. Similar conclusions are drawn by repeated simulations of t-tests. These mimic what is done in real life, which makes the results more persuasive. The simulation method is used also to evaluate the extent to which effect sizes are over-estimated, especially in underpowered experiments. A script is supplied to allow the reader to do simulations themselves, with numbers appropriate for their own work. It is concluded that if you wish to keep your false discovery rate below 5\%, you need to use a three-sigma rule, or to insist on p{$\leq$}0.001. And never use the word `significant'.},
  keywords = {false discovery rate,reproducibility,significance tests,statistics}
}

@article{Colquhoun:2019,
  title = {The {{False Positive Risk}}: {{A Proposal Concerning What}} to {{Do About}} p-{{Values}}},
  shorttitle = {The {{False Positive Risk}}},
  author = {Colquhoun, David},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {192--201},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1529622},
  urldate = {2024-03-20},
  abstract = {It is widely acknowledged that the biomedical literature suffers from a surfeit of false positive results. Part of the reason for this is the persistence of the myth that observation of p {$<$} 0.05 is sufficient justification to claim that you have made a discovery. It is hopeless to expect users to change their reliance on p-values unless they are offered an alternative way of judging the reliability of their conclusions. If the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. One such proposal is based on calculation of false positive risk(FPR). It is suggested that p-values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p-value. This number could be the minimum FPR (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). Alternatively one could specify the prior probability that it would be necessary to believe in order to achieve an FPR of, say, 0.05.},
  keywords = {Bayes,False positive,False positive report probability,False positive risk,FPR,Likelihood ratio,Point null,Positive predictive value}
}

@article{Gannonde_Braganca_PereiraPolpo:2019,
  title = {Blending {{Bayesian}} and {{Classical Tools}} to {{Define Optimal Sample-Size-Dependent Significance Levels}}},
  author = {Gannon, Mark Andrew and {de Bragan{\c c}a Pereira}, Carlos Alberto and Polpo, Adriano},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {213--222},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1518268},
  urldate = {2024-03-20},
  abstract = {This article argues that researchers do not need to completely abandon the p-value, the best-known significance index, but should instead stop using significance levels that do not depend on sample sizes. A testing procedure is developed using a mixture of frequentist and Bayesian tools, with a significance level that is a function of sample size, obtained from a generalized form of the Neyman--Pearson Lemma that minimizes a linear combination of {$\alpha$}, the probability of rejecting a true null hypothesis, and {$\beta$}, the probability of failing to reject a false null, instead of fixing {$\alpha$} and minimizing {$\beta$}. The resulting hypothesis tests do not violate the Likelihood Principle and do not require any constraints on the dimensionalities of the sample space and parameter space. The procedure includes an ordering of the entire sample space and uses predictive probability (density) functions, allowing for testing of both simple and compound hypotheses. Accessible examples are presented to highlight specific characteristics of the new tests.},
  keywords = {Hardy-Weinberg equilibrium,Neyman-Pearson lemma,Predictive distribution,Significance test}
}

@article{GoodmanSpruillKomaroff:2019,
  title = {A {{Proposed Hybrid Effect Size Plus}} P-{{Value Criterion}}: {{Empirical Evidence Supporting}} Its {{Use}}},
  shorttitle = {A {{Proposed Hybrid Effect Size Plus}} P-{{Value Criterion}}},
  author = {Goodman, William M. and Spruill, Susan E. and Komaroff, Eugene},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {168--185},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1564697},
  urldate = {2024-03-20},
  abstract = {When the editors of Basic and Applied Social Psychology effectively banned the use of null hypothesis significance testing (NHST) from articles published in their journal, it set off a fire-storm of discussions both supporting the decision and defending the utility of NHST in scientific research. At the heart of NHST is the p-value which is the probability of obtaining an effect equal to or more extreme than the one observed in the sample data, given the null hypothesis and other model assumptions. Although this is conceptually different from the probability of the null hypothesis being true, given the sample, p-values nonetheless can provide evidential information, toward making an inference about a parameter. Applying a 10,000-case simulation described in this article, the authors found that p-values' inferential signals to either reject or not reject a null hypothesis about the mean ({$\alpha$} = 0.05) were consistent for almost 70\% of the cases with the parameter's true location for the sampled-from population. Success increases if a hybrid decision criterion, minimum effect size plus p-value (MESP), is used. Here, rejecting the null also requires the difference of the observed statistic from the exact null to be meaningfully large or practically significant, in the researcher's judgment and experience. The simulation compares performances of several methods: from p-value and/or effect size-based, to confidence-interval based, under various conditions of true location of the mean, test power, and comparative sizes of the meaningful distance and population variability. For any inference procedure that outputs a binary indicator, like flagging whether a p-value is significant, the output of one single experiment is not sufficient evidence for a definitive conclusion. Yet, if a tool like MESP generates a relatively reliable signal and is used knowledgeably as part of a research process, it can provide useful information.},
  keywords = {meaningful distance,MESP,Minimum effect size plus p-value criterion,NHST,statistical evidence,true power,true Type I error rate}
}

@misc{Harrell:2017,
  title = {A {{Litany}} of {{Problems With}} P-Values},
  author = {Harrell, Frank},
  year = {2017},
  month = feb,
  journal = {Statistical Thinking},
  urldate = {2024-03-19},
  abstract = {p-values are very often misinterpreted. p-values and null hypothesis significant testing have hurt science. This article attempts to catalog all the ways in which these happen.},
  howpublished = {https://www.fharrell.com/post/pval-litany/},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/2XG9ZRPI/pval-litany.html}
}

@misc{iRF1:1913,
  title = {An Image of {{Ronald Fisher}} in 1913},
  shorttitle = {File},
  year = {1913},
  month = jan,
  journal = {Wikipedia},
  urldate = {2024-03-19},
  howpublished = {https://commons.wikimedia.org/wiki/File:Youngronaldfisher2.JPG},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/R3IXSBSL/FileYoungronaldfisher2.html}
}

@article{KrzywinskiAltman:2013,
  title = {Power and Sample Size},
  author = {Krzywinski, Martin and Altman, Naomi},
  year = {2013},
  month = dec,
  journal = {Nature Methods},
  volume = {10},
  number = {12},
  pages = {1139--1140},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2738},
  urldate = {2024-03-19},
  abstract = {The ability to detect experimental effects is undermined in studies that lack power.},
  copyright = {2013 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Publishing,Research data,Statistical methods},
  file = {/Users/javiermtz/Zotero/storage/6U89J62R/nmeth.html}
}

@article{LeekPeng:2015,
  title = {Statistics: {{P}} Values Are Just the Tip of the Iceberg},
  shorttitle = {Statistics},
  author = {Leek, Jeffrey T. and Peng, Roger D.},
  year = {2015},
  month = apr,
  journal = {Nature},
  volume = {520},
  number = {7549},
  pages = {612--612},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/520612a},
  urldate = {2024-03-20},
  abstract = {Ridding science of shoddy statistics will require scrutiny of every step, not merely the last one, say Jeffrey T. Leek and Roger D. Peng.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Careers,Mathematics and computing,Research management}
}

@misc{Leif:2023,
  title = {[114] {{Exhibits}} 3, 4, and 5},
  author = {Leif, Joe, \&, Uri},
  year = {2023},
  month = sep,
  journal = {Data Colada},
  urldate = {2024-03-20},
  abstract = {We recently presented evidence of data tampering in four retracted papers co-authored by Harvard Business School professor Francesca Gino. She is now suing the three of us (and Harvard University). Gino's lawsuit (.htm), like many lawsuits, contains a number of Exhibits that present information relevant to the case. For example, the lawsuit contains some Exhibits...},
  howpublished = {http://datacolada.org/114},
  langid = {american},
  file = {/Users/javiermtz/Zotero/storage/RYTFZ9W4/114.html}
}

@misc{Leif:2023a,
  title = {[112] {{Data Falsificada}} ({{Part}} 4): "{{Forgetting The Words}}"},
  shorttitle = {[112] {{Data Falsificada}} ({{Part}} 4)},
  author = {Leif, Joe, \&, Uri},
  year = {2023},
  month = jun,
  journal = {Data Colada},
  urldate = {2024-03-20},
  abstract = {This is the last post in a four-part series detailing evidence of fraud in four academic papers co-authored by Harvard Business School Professor Francesca Gino. It is worth reiterating two things. First, to the best of our knowledge, none of Gino's co-authors carried out or assisted with the data collection for the studies in this...},
  howpublished = {https://datacolada.org/112},
  langid = {american},
  file = {/Users/javiermtz/Zotero/storage/UYV6AICB/112.html}
}

@article{Matthews:2019,
  title = {Moving {{Towards}} the {{Post}} p {$<$} 0.05 {{Era}} via the {{Analysis}} of {{Credibility}}},
  author = {Matthews, Robert A. J.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {202--212},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1543136},
  urldate = {2024-03-20},
  abstract = {It is now widely accepted that the techniques of null hypothesis significance testing (NHST) are routinely misused and misinterpreted by researchers seeking insight from data. There is, however, no consensus on acceptable alternatives, leaving researchers with little choice but to continue using NHST, regardless of its failings. I examine the potential for the Analysis of Credibility (AnCred) to resolve this impasse. Using real-life examples, I assess the ability of AnCred to provide researchers with a simple but robust framework for assessing study findings that goes beyond the standard dichotomy of statistical significance/nonsignificance. By extracting more insight from standard summary statistics while offering more protection against inferential fallacies, AnCred may encourage researchers to move toward the post p {$<$} 0.05 era.},
  keywords = {Analysis of credibility,Bayesian inference,Null hypothesis Significance testing,p-Values}
}

@article{Nuzzo:2013,
  title = {Online Daters Do Better in the Marriage Stakes},
  author = {Nuzzo, Regina},
  year = {2013},
  month = jun,
  journal = {Nature},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature.2013.13120},
  urldate = {2024-03-19},
  abstract = {Those who first find each other through the Internet are more likely to stay hitched.},
  copyright = {2013 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Psychology,Society,Sociology},
  file = {/Users/javiermtz/Zotero/storage/PFR2UB3N/nature.2013.html}
}

@article{Nuzzo:2014,
  title = {Scientific Method: {{Statistical}} Errors},
  shorttitle = {Scientific Method},
  author = {Nuzzo, Regina},
  year = {2014},
  month = feb,
  journal = {Nature},
  volume = {506},
  number = {7487},
  pages = {150--152},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/506150a},
  urldate = {2024-03-19},
  abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
  copyright = {2014 Springer Nature Limited},
  langid = {english},
  keywords = {Lab life,Mathematics and computing,Medical research,Publishing},
  file = {/Users/javiermtz/Zotero/storage/7PNWBME6/506150a.html}
}

@article{Oscbtec:2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  urldate = {2024-03-19},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  langid = {english}
}

@misc{P:2024,
  title = {P-{{Values}}},
  journal = {xkcd},
  urldate = {2024-03-15},
  howpublished = {https://xkcd.com/1478/},
  file = {/Users/javiermtz/Zotero/storage/WYU44NEP/1478.html}
}

@article{Pogrow:2019,
  title = {How {{Effect Size}} ({{Practical Significance}}) {{Misleads Clinical Practice}}: {{The Case}} for {{Switching}} to {{Practical Benefit}} to {{Assess Applied Research Findings}}},
  shorttitle = {How {{Effect Size}} ({{Practical Significance}}) {{Misleads Clinical Practice}}},
  author = {Pogrow, Stanley},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {223--234},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1549101},
  urldate = {2024-03-20},
  abstract = {Relying on effect size as a measure of practical significance is turning out to be just as misleading as using p-values to determine the effectiveness of interventions for improving clinical practice in complex organizations such as schools. This article explains how effect sizes have misdirected practice in education and other disciplines. Even when effect size is incorporated into RCT research the recommendations of whether interventions are effective are misleading and generally useless to practitioners. As a result, a new criterion of practical benefit is recommended for evaluating research findings about the effectiveness of interventions in complex organizations where benchmarks of existing performance exist. Practical benefit exists when the unadjusted performance of an experimental group provides a noticeable advantage over an existing benchmark. Some basic principles for determining practical benefit are provided. Practical benefit is more intuitive and is expected to enable leaders to make more accurate assessments as to whether published research findings are likely to produce noticeable improvements in their organizations. In addition, practical benefit is used routinely as the research criterion for the alternative scientific methodology of improvement science that has an established track record of being a more efficient way to develop new interventions that improve practice dramatically than RCT research. Finally, the problems with practical significance suggest that the research community should seek different inferential methods for research designed to improve clinical performance in complex organizations, as compared to methods for testing theories and medicines.},
  keywords = {Applied research,Evidence-based decision-making,Improvement science,Interpreting effect sizes,Leadership-decision-making,Organizational effectiveness}
}

@misc{Resnick:2017a,
  title = {What a Nerdy Debate about P-Values Shows about Science --- and How to Fix It},
  author = {Resnick, Brian},
  year = {2017},
  month = jul,
  journal = {Vox},
  urldate = {2024-03-20},
  abstract = {The case for, and against, redefining "statistical significance."},
  howpublished = {https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/SUNDRIGA/p-values-statistical-significance-redefine-0005.html}
}

@misc{S:2024,
  title = {Significant},
  journal = {xkcd},
  urldate = {2024-03-15},
  howpublished = {https://xkcd.com/882/},
  file = {/Users/javiermtz/Zotero/storage/YPDN2T3E/882.html}
}

@misc{Scheiber:2023,
  title = {The {{Harvard Professor}} and the {{Bloggers}}},
  author = {Scheiber, Noam},
  year = {2023},
  month = sep,
  urldate = {2024-03-20},
  abstract = {When Francesca Gino, a rising academic star, was accused of falsifying data --- about how to stop dishonesty --- it didn't just torch her career. It inflamed a crisis in behavioral science.},
  chapter = {Business},
  langid = {american},
  keywords = {Academic and Scientific Journals,Ariely Dan,Colleges and Universities,Ethics and Official Misconduct,Falsification of Data,Gino Francesca,Harvard Business School,Libel and Slander,Research,Sociology,Suits and Litigation (Civil)}
}

@misc{Simonsohn:2020,
  title = {[91] p-Hacking Fast and Slow: {{Evaluating}} a Forthcoming {{AER}} Paper Deeming Some Econ Literatures Less Trustworthy},
  shorttitle = {[91] p-Hacking Fast and Slow},
  author = {Simonsohn, Uri},
  year = {2020},
  month = sep,
  journal = {Data Colada},
  urldate = {2024-03-19},
  abstract = {The authors of a forthcoming AER article (.pdf), "Methods Matter: P-Hacking and Publication Bias in Causal Analysis in Economics", painstakingly harvested thousands of test results from 25 economics journals to answer an interesting question: Are studies that use some research designs more trustworthy than others? In this post I will explain why I think their...},
  howpublished = {https://datacolada.org/91},
  langid = {american},
  file = {/Users/javiermtz/Zotero/storage/QWLWAM79/91.html}
}

@article{StefanSchonbrodt:2023,
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, Angelika M. and Sch{\"o}nbrodt, Felix D.},
  year = {2023},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220346},
  urldate = {2024-03-20},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  keywords = {error rates,false-positive rate,p-curve,questionable research practices,Shiny app,significance,simulation}
}

@article{WassersteinLazar:2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2024-03-19}
}

@misc{Yrptvaw0:2015,
  title = {You're Probably Thinking of p Values All Wrong (p {$<$} 0.05)},
  year = {2015},
  month = nov,
  journal = {The Incidental Economist},
  urldate = {2024-03-19},
  abstract = {I'm guilty of getting this wrong, and it is likely you are too: If the p-value is},
  file = {/Users/javiermtz/Zotero/storage/CV3FQVJT/youre-probably-thinking-of-p-values-all-wrong-p-0-05.html}
}
